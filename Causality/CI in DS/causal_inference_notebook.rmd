---
title: "Rhyme Project Causal Inference - Empty Notebook"
output: 
  html_document:
    toc: true
    number_sections: false
    code_folding: hide
    theme: cosmo
date: "`r format(Sys.time(), '%d %B, %Y')`"
editor_options: 
  chunk_output_type: console
---

## Load Necessary Packages & Helper Functions

```{r setup, include=FALSE}
setwd("/Desktop")
source('causal_inference_helper_functions.R')
print("Setup Complete")
```

## Controlled / Fixed Effects Regression

```{r}
# function to simulate data set
dat<-sim.fixed.effects.df()

# explore data
colnames(dat)

head(dat)
```

```{r}
# Goal: Find the impact of "X = satisfaction = Customer.Rating" on "Y = spend = Customer.Spend"

# Customer.Spend vs. Customer.Rating:

g1<-dat %>%
  ggplot(aes(Customer.Rating,Customer.Spend,fill="A")) +
  geom_point() +
  theme_economist() +
  scale_fill_economist()

g1
cor(dat$Customer.Rating,dat$Customer.Spend)
```

```{r}
# customer Spend vs. time
g2<-dat %>%
  ggplot(aes(Time.FE,Customer.Spend,fill="A")) +
  geom_boxplot() +
  theme_economist() +
  scale_fill_economist() +
  theme(legend.position="none",axis.text.x=element_text(angle=45,
                                                        vjust=0.5))

# customer spend vs. product
g3<-dat %>%
  ggplot(aes(Product.FE,Customer.Spend,fill="A")) +
  geom_boxplot() +
  theme_economist() +
  scale_fill_economist() +
  theme(legend.position="none",axis.text.x=element_text(angle=45,
                                                        vjust=0.5))

# Customer spend vs. customer Age
g4<-dat %>%
  ggplot(aes(Customer.Age,Customer.Spend,fill="A")) +
  geom_point() +
  theme_economist() +
  scale_fill_economist() +
  theme(legend.position="none")

# Customer spend vs. total purchases
g5<-dat %>%
  ggplot(aes(Total.Purchases,Customer.Spend,fill="A")) +
  geom_point() +
  theme_economist() +
  scale_fill_economist() +
  theme(legend.position="none")

# combined all plots
grid.arrange(g2,g3,g4,g5,nrow=2)

# observe differences across time & products & customer age
# need to include in regression to control for their effects
```

```{r warning=F}
# compare controlled reg/fixed effect models

# naive regression; no controls
model1<- lm(Customer.Spend~Customer.Rating, data=dat)

# control for customer age only
model2<- lm(Customer.Spend~Customer.Rating+Customer.Age, data=dat)

# control for product and time fixed effects only
model3<- lm(Customer.Spend~Customer.Rating+Product.FE+Time.FE, data=dat)

# full controls; and included variable bias of total purchases
model4<- lm(Customer.Spend~Customer.Rating+Customer.Age+Product.FE+Time.FE+Total.Purchases, data=dat)

# full controls; no included variable bias
model5<- lm(Customer.Spend~Customer.Rating+Customer.Age+Product.FE+Time.FE, data=dat)
```

```{r}
# see here for more info on stargazer:
# (1) https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf
# (2) https://www.jakeruss.com/cheatsheets/stargazer/

# view coefficient of interest in each regression model
stargazer(model1,model2,model3,model4,model5,type="text",
          style="aer",omit=c("Constant","Customer.Age",
                             "Product.FE","Time.FE",
                             "Total.Purchases"),
          column.labels=c("Y~X","Y~X+C","Y~X+FE","Y~X+C+FE+IVB",
                          "Y~X+C+FE"),
          dep.var.labels="Controlled / Fixed Effects Regression",
          omit.stat=c("f","ser","rsq","n"),
          notes=c("True Coef on customer rate X = 2"),
          add.lines=list(c("Add. Controls","No","Yes","No",
                           "Yes","Yes"),
                         c("Fixed effects","No","No","Yes",
                           "Yes","Yes"),
                         c("Included Variable Bias","No","No","No",
                           "Yes","No")))

# l1 is a very bad model: 1) coefficient of X = 4.93 and is far from true value 2 2) R2 is very low: we have "omitted variable bias"
# l4 is overfitted and have "including variable bias"= Total.Purchases: While R2 is very good, the coefficient of X = 2.45 differs from 2
# l5 is very good: Both R2 and coefficient of X are very good
```

## Regression Discontinuity

```{r}
# function to simulate data set
dat<-sim.reg.discontinuity.df()
colnames(dat)
head(dat)

# Goal: Find the impact of "X = additional customer support = Add.Support" on "Y = customer spend = Customer.Spend"

# We have a lead scoring ML model that outputs a score between 0 and 100. The score is based on the likelihood that we think a customer is new. Because this lead scoring model has previously worked well, we use it to decide whether we want to give additional support to customers or not.

# When the lead score > 70 we treated it as a high quality deal and provide additional customer support. This is specified by Add.Support variable.  So, the Add.Support is a binary variable that is True only when Lead.Score >= 70.

# We can assume that customers just below and just above the 70 cutoff for additional customer support are pretty similar to one another. Therefore, a discontinuity point that can be used to estimate the impact of Add.Support on Customer.Spend by comparing observations just below and above 70.

# The counterfactual column is the trend that we would see if the additional support on Customers.Spend, post the discontinuity cutoff, had no actual effect.
```

```{r}
# regression discontinuity plot
dat %>%
  ggplot(aes(Lead.Score,Customer.Spend,color=Add.Support)) +
  geom_line(lwd=2) +
  geom_line(aes(Lead.Score,Counterfactual),lty=2,lwd=2) +
  geom_vline(xintercept=dat$Lead.Score[sum(!dat$Add.Support)],
             lty=2,lwd=2) +
  xlab("Lead Score") +
  ylab("Customer Spend") +
  theme_economist() +
  scale_color_economist() +
  theme(legend.position="none")
```

```{r warning=F}
# fit regression discontinuity model
model1<-lm (Customer.Spend~Lead.Score+Add.Support+Lead.Score:Add.Support, data=dat)

#Y= B0+ B1 X+ B2 I(X>cutoff) + B3 X I(X>cutoff)
# Lead.Score:Add.Support is the interaction term between the two variables X and I(X>cutoff)
```

```{r}
# view regression discontinuity model
stargazer(model1,type="text",style="aer",
          column.labels=c("Y~X+I(X>Cutoff)+X*I(X>Cutoff)"),
          dep.var.labels="Regression Discontinuity",
          omit.stat=c("f","ser","rsq","n","adj.rsq"),
          intercept.bottom=F)

# We compare the two regression lines and see whether they are "statistically different" by checking whether the intercept and slope terms post cutoff are "statistically significant."

# B1 (coef of Lead.Score) is statistically significant (0.174). Hence, there is a relationship between X and Y.
# B2, (coef of Add.Support) is not significant. 
# But, B3 is significant implying that there is a different relationship pre and post cut off, suggesting that Add.Support has a causal impact on Customer.Spend.

coef(model1)
# B0 = -5.7 , B1 = 2.1, B2 = 20.1, B3= 2.07

# Causal Impact = difference between the regression lines at cutoff point
coef(model1)["Add.SupportTRUE"] + coef(model1)["Lead.Score:Add.SupportTRUE"]* 70
# Causal Impact= B2+ B3 * 70=165.38
```

## Difference in Difference


```{r}
# function to simulate data set
dat<-sim.diff.in.diff.df()

# explore data
colnames(dat)

head(dat)

# Goal: to estimate the impact of increasing price on Y= Revenue.

#This is a common scenario that AB testing cannot be done. So, we apply the difference in difference method.

# We decide to only change the prices in Australia (raising them by $20 starting in January 2019) and keep the US prices the same. 

# Hence, Australia and the US are respectively our treatment and controlled markets.

# We gather monthly revenue for each market over 2018 and 2019 and use this to estimate the impact of changing prices on revenue.

# In this dataset, we have a time period column, an indicator for the time period and whether it is pre or post the intervention in January 2019.

# The counterfactual is what we would expect if the price change had no effect. And it is based on the trend observed in the controlled market (the US).
```

```{r}
# difference in difference plot
dat %>%
  ggplot(aes(Time,Revenue,color=Country)) +
  geom_line(lwd=2) +
  geom_line(aes(Time,Counterfactual),lty=2,lwd=2) +
  xlab("Time") +
  ylab("Revenue") +
  theme_economist() +
  scale_color_economist()
```

```{r warning=F}
# fit difference in difference model
model1<- lm(Revenue~Period+Country+Period:Country, data=dat)
# Country =! 0 if it is AU
# Period =! 0 if it after intervention

# 4 estimated revenue for 4 scenarios:
# Rev in US pre Intervention = Intercept (Period, Country, Interaction = 0)
# Rev in AU pre Intervention = Intercept + Country (Period, Interaction = 0)
# Rev in US post Intervention = Intercept + Period (Country, Interaction = 0)
# Rev in AU post Intervention = Intercept + Period + Country + Interaction

# Diff in Diff = (Rev in AU Post Intervention - Rev in AU Pre Intervention) - (Rev in US Post Intervention - Rev in US Pre Intervention) = (Intercept + Period + Country + Interaction - Intercept + Country) - (Intercept + Period - Intercept) = Interaction
```

```{r}
# view difference in difference model
stargazer(model1,type="text",style="aer",
          column.labels=c("Y~Post+G+Post*G"),
          dep.var.labels="Difference in Difference",
          omit.stat=c("f","ser","rsq","n","adj.rsq"),
          notes=c("Causal Impact = 100"),intercept.bottom=F)

```

## Instrumental Variable

```{r warning=F}
# function to simulate data set
dat<-sim.iv.df()

# explore data
colnames(dat)

head(dat)

# Goal: to understand the relationship between "X = using mobile app" and "Y = customer retention".

# Direct correlation between X and Y is potentially biased, because motivated users are both more likely to use the mobile app and to retain. We also can't use controlled regression, because we have not a proxy for "motivation".

# 4 columns: 
#Use.Mobile.App which is a 01 indicator for using the mobile app. 
#Retention which is a 01 indicator for customer attention. 
#Received.Email which is going to be our IV. 
#Unobs.Motivation which is a motivation score that we cannot observe.


# We randomly send one of two email variants: 1) a control email that does not mention the mobile app--> Received.Email=0 2) Treatment email that mentions the mobile app -->Received.Email=1.

# Received.Email is hence an IV variable that should be randomized. It is also uncorrelated with Unobs.Motivation. But, we should check whether it satisfies the Strong First Stage assumption.


#The following lines show that Unobs.Motivation is a confounder for both X and Y: 

tapply(dat$Unobs.Motivation,dat$Use.Mobile.App,mean)
tapply(dat$Unobs.Motivation,dat$Retention,mean)
```

```{r warning=F}
# fit IV model

# Naive Regression Y=f(X)
model1<- lm(Retention~Use.Mobile.App, data=dat)

# First-Stage Regression: Regress X on IV variable X=f(Z)
model2<-lm(Use.Mobile.App~Received.Email, data=dat)

# Second-Stage Regression: Regress Y on the predictions of the first-stage regression
model3<-lm(Retention~predict(model2), data=dat)

# Two Stage Least Squares (TSLS) for IV: Combine the two steps and do them simultaneously. 
model4<-ivreg(Retention~Use.Mobile.App|Received.Email,data=dat)

```

```{r}
# compare all models
stargazer(model1,model2,model3,model4,type="text",style="aer",
          column.labels=c("Y~X","X~Z","Y~Xhat","IV"),
          omit=c("Constant"),
          dep.var.labels=c("Retention","Use.Mobile.App",
                           "Retention","Retention"),
          covariate.labels=c("Use.Mobile.App","Received.Email",
                             "Use.Mobile.App.Hat"),
          model.names=F,omit.stat=c("ser","rsq","n","adj.rsq"),
          intercept.bottom=F)

# The first column suggests that using the mobile app would increase customer retention. But it is inaccurate because of suffering from omitted variable bias.

# The second column says that receiving an email is significantly related to using the mobile app with an F statistic above 14.

# The third column represents the causal impact of using the mobile app on retention. It is positive and statistically significant, suggesting that there is a real impact to using the mobile app on retention.

# The fourth column is exactly the same as the results from our third column.
```

## Double Selection

```{r warning=F}
# function to simulate data set
dat<-sim.double.selection.df()

# explore data
dim(dat)

colnames(dat[,1:10])

head(dat[,1:10])

# We are interested in analyzing the results from an AB test on a social group. 

# We randomly assigned some customers a variant in which they are shown testimonials from other customers on their satisfaction with their purchase. There was also a control variant without testimonials. Goal: to know whether showing testimonials increases customer value.

# This is an AB test, so we can just look at the average difference between treatment and control groups. But, using double selection can reduce statistical noise in our estimate of the causal impact and allow us to have a more precise estimate and faster time to resolution in the test.

# We have 1000 rows/samples and 502 columns/variables.

# Y = Customer.Value
# X = Social.Proof.Variant


```

```{r warning=F}
# Fiting the double selection model

# 1) isolate control variables

C<-dat[, -which(colnames(dat)%in%c("Customer.Value", "Social.Proof.Variant"))]

C<-as.matrix.data.frame(C)


# 2) regress Y on C variables using lasso

y.glmnet.model<-cv.glmnet(C, dat$Customer.Value)


# 3) extract the nonzero coefficients from the above lasso and use lambda min CV within 1 se (select less coef)

nonzero<-unlist(predict(y.glmnet.model,s="lambda.1se",type="nonzero"))
Y.on.C<-colnames(C)[nonzero] # these are the variables in C, which are important for predicting Y 
Y.on.C

# 4) fit lasso regressing treatment on control variables (regress X on C variables using lasso)

x.glmnet.model<-cv.glmnet(C,dat$Social.Proof.Variant)


# 5) extract nonzero coefficients from lasso above and use lambda min CV within 1 se (select less coeff)

nonzero<-unlist(predict(x.glmnet.model,s="lambda.1se",type="nonzero"))
X.on.C<-colnames(C)[nonzero]
X.on.C # There is no variable in C which is important for predicting X


# 6) combine two sets of nonzero coefficients to get unique nonzero coefficients across models

var.union<-unique(c(Y.on.C,X.on.C))

# 7) count number of nonzero variables

length(var.union)
var.union # Among the 500 variables in C, only 9 variables are important


# 8) Finally, regress Y on X plus the nonzero-coef variables in C to obtain a double selection regression model:

double.selection<-lm(Customer.Value~.,dat[,c("Customer.Value","Social.Proof.Variant",var.union)])
```

```{r}
# compare naive model, full model, and double selection

# naive regression (regressing Y only on X):
naive.regression<-lm(Customer.Value~Social.Proof.Variant,data=dat)

# regression with full controls (regressing Y on all 501 variables):
full.model<-lm(Customer.Value~.,data=dat)
```

```{r}
# compare the 3 models:
stargazer(naive.regression,full.model,double.selection,type="text",style="aer",
          column.labels=c("No Controls","All Controls",
                          "Double Selection"),
          dep.var.labels=c(""),
          covariate.labels=c("Social.Proof.Variant"),
          omit=c("V[0-9]","Constant"),
          model.names=F,omit.stat=c("ser","rsq","n","adj.rsq","F"),
          notes=c("Causal Impact = 2"))
```

## Causal Forests

```{r}
# function to simulate data set
dat<-sim.causal.forest.df()

# explore data
colnames(dat)

head(dat)

# We have historical data on customer revenue and whether they have received a discount in the past or not, along with a set of 5 control variables in V and a column for registration source (which is the Social Media channel each customer created their account from).

# Goal: to know whether giving a Discount has an impact on customer Revenue, and whether this impact differs by registration source.

# Hence, we're interested in heterogeneous treatment effects, where the treatment effect differs by characteristics of the observation.

# Y= Revenue, X = Discount  

table(dat$Registration.Source)

# There are 4 Registration.Source values: Google, Instagram, Twitter, Bing
```

```{r warning=F}
# Regular OLS: Regress Y on all variables expect for Registration.Source
model1<-lm(Revenue~.,data=dat[,-which(colnames(dat)=="Registration.Source")])

# OLS with interactions for heterogeneous treatments by Registration.Source
model2<-lm(Revenue~.+Discount*Registration.Source,data=dat)
```

```{r}
# compare OLS models
stargazer(model1,model2,type="text",style="aer",
          column.labels=c("All Controls",
                          "All Controls + Group Interactions"),
          dep.var.labels=c("","",""),
          covariate.labels=c("Discount",
                             "Discount x Instagram",
                             "Discount x Twitter",
                             "Discount x Bing"),
          omit=c("V","Constant","^Registration.Source"),
          model.names=F,omit.stat=c("ser","rsq","n","adj.rsq"))

# So the base group is the Google, which has an impact of 5 compared to the impacts of 10 for Instagram, 15 for Twitter, and 20 for Bing.

# If we average these different causal impacts, we find 12.46. However, because these interaction terms are significant, we know that there is a significant heterogeneity in our treatment effects.

# Hence, we fit a causal forest model and see if the causal forest can learn this heterogeneity.
```

```{r warning=F}
# Fit a causal forest model
Vars<-model.matrix(~.,data=dat[,-which(colnames(dat)%in%c("Revenue","Discount"))]) # Vars= all variables except X and Y

cf<-causal_forest(X=Vars,Y=dat$Revenue,W=dat$Discount)
```

```{r}
# obtain causal forest model predictions
pred<-predict(cf)$predictions

# view average causal forest model predictions by Registration.Source to estimate the heterogeneous treatment effect
tapply(pred,dat$Registration.Source,mean) # Google (4.7) Instagram (10.6) Twitter (15.01) Bing (19.9)     

# Feature Importance: Bing and Twitter are the most important features which have the largest heterogeneous treatment effect
cf %>% 
  variable_importance() %>% 
  as.data.frame() %>% 
  mutate(variable=colnames(Vars)) %>% 
  arrange(desc(V1))

# plot distribution of causal forest model predictions by sources
data.frame("est"=pred,"Registration.Source"=dat$Registration.Source) %>%
  ggplot(aes(Registration.Source,pred,fill=Registration.Source)) +
  geom_boxplot() +
  xlab("Registration Source") +
  ylab("Estimated Treatment") +
  theme_economist() +
  scale_fill_economist() +
  theme(legend.position="none")
```
