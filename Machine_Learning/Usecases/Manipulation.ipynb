{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.Connection(\"./switrs.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color:red;font-size:2rem;\">WARNING: This notebook will use a lot of RAM!</strong>\n",
    "    \n",
    " This notebook will use about 8-10 gigs of RAM loading the two tables into dataframes.\n",
    " \n",
    " If you want to use less, make this number below larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDUCTION_FACTOR = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM collisions WHERE case_id % 10 = 0': no such table: collisions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: collisions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aaee3cad560c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"SELECT * FROM collisions WHERE case_id % {REDUCTION_FACTOR} = 0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"collision_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQLiteDatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM collisions WHERE case_id % 10 = 0': no such table: collisions"
     ]
    }
   ],
   "source": [
    "collisions = pd.read_sql(f\"SELECT * FROM collisions WHERE case_id % {REDUCTION_FACTOR} = 0\", con, parse_dates=[\"collision_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parties with the same case_id as selected for collisions\n",
    "parties = pd.read_sql(f\"SELECT * FROM parties WHERE case_id % {REDUCTION_FACTOR} = 0\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Interview Practice: Data Manipulation\n",
    "\n",
    "I often get asked by newly-minted PhDs trying to get their first data job:\n",
    "\n",
    "> How can I prepare for dataset-based interviews? Do you have any examples of\n",
    "> datasets to practice with?\n",
    "\n",
    "I never had a good answer. I would tell them about how the interviews worked,\n",
    "but I wished I had something to share that they could get their hands on.\n",
    "\n",
    "As of today, that's changing. In this post I put together a series of practice\n",
    "questions like the kind you might see (or be expected to come up with) in a\n",
    "hands-on data interview using the [curated and hosted dataset of California\n",
    "Traffic accidents][switrs_dataset]. The dataset is available for download from\n",
    "both [Kaggle][kaggle] and [Zenodo][zenodo], and I even have an [example\n",
    "notebook][example_notebook] demonstrating how to work with the data entirely\n",
    "online within Kaggle.\n",
    "\n",
    "[switrs_dataset]: https://alexgude.com/blog/switrs-sqlite-hosted-dataset/\n",
    "\n",
    "[kaggle]: https://www.kaggle.com/alexgude/california-traffic-collision-data-from-switrs\n",
    "\n",
    "[zenodo]: https://zenodo.org/record/4284843\n",
    "\n",
    "[example_notebook]: https://www.kaggle.com/alexgude/starter-california-traffic-collisions-from-switrs\n",
    "\n",
    "## Interview Format\n",
    "\n",
    "As I mentioned in [my post about my most recent interview\n",
    "experience][last_post], data science and machine learning interviews have\n",
    "become more practical, covering tasks that show up in the day-to-day work of a\n",
    "data scientist instead of hard but irrelevant problems. One common interview\n",
    "type involves working with a dataset, answering some simple questions about\n",
    "it, and then building some simple features.\n",
    "\n",
    "[last_post]: https://alexgude.com/blog/interviewing-for-data-science-positions-in-2020/\n",
    "\n",
    "Generally these interviews use Python and [Pandas][pandas] or pure SQL.\n",
    "Sometimes the interviewer has a set of questions for you to answer and\n",
    "sometimes they want you to come up with your own.\n",
    "\n",
    "[pandas]: https://en.wikipedia.org/wiki/Pandas_(software)\n",
    "\n",
    "To help people prepare, I have created a set of questions similar to what you\n",
    "would get in a real interview. For the exercise you will be using the SWITRS\n",
    "dataset. I have included a notebook to get you started in Pandas or SQL. The\n",
    "solution notebooks can be found at the very end.\n",
    "\n",
    "Good luck, and if you have any questions or suggestions please reach out to me\n",
    "on Twitter: [@alex_gude][twitter]\n",
    "\n",
    "[twitter]: https://twitter.com/alex_gude\n",
    "\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many collisions are there in the dataset?\n",
    "\n",
    "A good first thing to check is \"How much data am I dealing with?\"\n",
    "\n",
    "Each row in the collisions database represents one collision, so the solution\n",
    "is nice and short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(collisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What percent of collisions involve males aged 16--25?\n",
    "\n",
    "Young men are famously unsafe drivers so let's look at how many collisions\n",
    "they're involved in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criteria = (\n",
    "    (parties[\"party_sex\"] == \"male\")\n",
    "    & (parties[\"party_age\"].between(16, 25))\n",
    ")\n",
    "\n",
    "# There are a lot of NULLs, which I assume are uncorrelated to \n",
    "# the real Age and Sex, so I remove them.\n",
    "denominator_criteria = (\n",
    "    (~parties[\"party_sex\"].isna())\n",
    "    & (~parties[\"party_age\"].isna())\n",
    ")\n",
    "\n",
    "len(parties[criteria][\"case_id\"].unique()) / len(parties[denominator_criteria][\"case_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't match the answer in the post because we are randomly sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many solo motorcycle crashes are there per year?\n",
    "\n",
    "A _\"solo\"_ crash is one where the driver runs off the road or hits a\n",
    "stationary object. How many solo motorcycle crashes were there each year? Why\n",
    "does 2020 seem to (relatively) have so few?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = (\n",
    "    (collisions[\"party_count\"] == 1)  # Solo crash\n",
    "    & (collisions[\"motorcycle_collision\"] == True)  # Is motorcycle collision\n",
    ")\n",
    "\n",
    "# Select a single column to speed computation, it could be any column, but I used jurisdiction.\n",
    "collisions[criteria][\"jurisdiction\"].groupby(collisions[\"collision_date\"].dt.year).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count is low in 2020 primarily because the data doesn't cover the whole\n",
    "year. It is also low due to the COVID pandemic keeping people off the streets,\n",
    "at least initially. To differentiate these two causes we could compare month\n",
    "by month to last year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What make of vehicle has the largest fraction of accidents on the weekend? During the work week?\n",
    "\n",
    "Weekdays are generally commute and work-related traffic, while weekends\n",
    "involves recreational travel. Do we see different vehicles involved in\n",
    "collisions on these days?\n",
    "\n",
    "Only consider vehicle makes with at least 1,000 collisions, in order to focus\n",
    "only on common vehicles where the difference between weekend and weekday usage\n",
    "will be significant.\n",
    "\n",
    "This query is tricky. We need to aggregate collisions by vehicle make, which\n",
    "means we need the parties table. We also care about when the crash happened,\n",
    "which means we need the collisions table. So we need to join these two tables\n",
    "together.\n",
    "\n",
    "In an interview setting, I would write two simpler queries: one\n",
    "that gets the highest weekend fraction and one that gets the highest weekday\n",
    "fraction with a lot of copy and pasted code. This is a lot easier to work out.\n",
    "Here is an example of one of those queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collisions[\"day_name\"] = collisions[\"collision_date\"].dt.day_name()\n",
    "collisions[\"is_weekend\"] = collisions[\"day_name\"].isin((\"Saturday\", \"Sunday\"))\n",
    "collisions[\"is_weekday\"] = ~collisions[\"is_weekend\"]\n",
    "\n",
    "merged = pd.merge(\n",
    "    parties[[\"case_id\", \"vehicle_make\"]], \n",
    "    collisions[[\"case_id\", \"is_weekend\", \"is_weekday\"]], \n",
    "    how=\"inner\",\n",
    "    on=[\"case_id\"],\n",
    ")\n",
    "\n",
    "grouped = merged[[\"vehicle_make\", \"is_weekend\", \"is_weekday\"]].groupby(\"vehicle_make\").sum()\n",
    "grouped[\"total\"] = grouped[\"is_weekend\"] + grouped[\"is_weekday\"]\n",
    "selected = grouped[grouped[\"total\"] >= 1_000].copy()\n",
    "\n",
    "selected[\"weekend_ratio\"] = selected[\"is_weekend\"] / selected[\"total\"]\n",
    "selected[\"weekday_ratio\"] = selected[\"is_weekday\"] / selected[\"total\"]\n",
    "\n",
    "top_weekend = selected.sort_values([\"weekend_ratio\"], ascending=False).head(1)\n",
    "top_weekday = selected.sort_values([\"weekday_ratio\"], ascending=False).head(1)\n",
    "\n",
    "answer = pd.concat([top_weekend, top_weekday])\n",
    "answer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many different values represent \"Toyota\" in the Parties database? How would you go about correcting for this?\n",
    "\n",
    "Data is **_never_** as clean as you would hope,  and this applies even to the\n",
    "[curated SWITRS dataset][switrs_dataset]. How many different ways does\n",
    "\"Toyota\" show up?\n",
    "\n",
    "[switrs_dataset]: https://alexgude.com/blog/switrs-sqlite-hosted-dataset/\n",
    "\n",
    "What steps would you take to fix this problem?\n",
    "\n",
    "\n",
    "This is a case where there is no _right_ answer. You can get a more and more\n",
    "correct answer as you spend more time, but at some point you have to decide it\n",
    "is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "regex = r\"(TOYOTA.*|TOY.*|TY.*)\"\n",
    "\n",
    "just_makes = parties[[\"vehicle_make\", \"party_sex\"]]\\\n",
    "               .groupby(\"vehicle_make\")\\\n",
    "               .count()\\\n",
    "               .reset_index()\\\n",
    "               .rename(columns={\"party_sex\": \"count\"})\\\n",
    "               .sort_values(\"count\", ascending=False)\n",
    "\n",
    "just_makes[just_makes[\"vehicle_make\"].str.match(regex)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of those look like they mean Toyota, although Tymco is a different\n",
    "company that makes street sweepers.\n",
    "\n",
    "Here is how I would handle this issue: the top 5 make up the vast majority of\n",
    "entries. I would fix those by hand and move on. More generally it seems that\n",
    "makes are represented mostly by their name or a four-letter abbreviation. It\n",
    "wouldn't be too hard to detect and fix these for the most common makes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
